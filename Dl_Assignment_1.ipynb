{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dl_Assignment-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKqUafkDdDsY5NxEgPR4zc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srimouli04/DSE/blob/master/Dl_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25__-ksw0rRg"
      },
      "source": [
        "# DL - Assignment - 1 \n",
        "\n",
        "##Group-13\n",
        "\n",
        "### Problem set -3\n",
        "\n",
        ">Borusu Naga Srimouli - 2019Ah04075\n",
        "\n",
        ">Srikar\n",
        "\n",
        ">Prakash\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orI50VZExHt7"
      },
      "source": [
        "# Question No.1. \n",
        "\n",
        "Vision Dataset:*CIFAR-10*- It dataset consists of 60000 32x32 colour images in 10 classes. Please find your dataset from the link- https://www.tensorflow.org/datasets/catalog/cifar10\n",
        "(6 marks)\n",
        "\n",
        "##Tasks:\n",
        "\n",
        "1. Import Libraries/Dataset (0 mark)\n",
        "\n",
        "    a. Import the required libraries and the dataset (use Google Drive if required).\n",
        "\n",
        "    b. Check the GPU available (recommended- use free GPU provided by Google Colab).\n",
        "2. Data Visualization and augmentation (1 mark)\n",
        "    \n",
        "    a. Plot at least two samples from each class of the dataset (use matplotlib/seaborn/any other library). \n",
        "    \n",
        "    b. Apply horizontal flip and width shift augmentation (horizontal_flip, width_shift_range) to the dataset separately. Print the augmented image and the original image for each class and each augmentation.\n",
        "\n",
        "    c. Bring the train and test data in the required format.\n",
        "    \n",
        "    d. Print the shapes of train and test data.\n",
        "3. Model Building (0.2*5 = 1 mark)\n",
        "\n",
        "  a. Sequential Model layers- Use AT LEAST 3 hidden layers with appropriate input for each. Choose the best number for hidden units and give reasons.\n",
        "\n",
        "  b. Add L2 regularization to all the layers.\n",
        "\n",
        "  c. Add one layer of dropout at the appropriate position and give reasons.\n",
        "\n",
        "  d. Choose the appropriate activation function for all the layers.\n",
        "\n",
        "  e. Print the model summary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Model Compilation (0.25 mark)\n",
        "\n",
        "  a. Compile the model with the appropriate loss function.\n",
        "\n",
        "  b. Use an appropriate optimizer. Give reasons for the choice of learning rate and its value.\n",
        "\n",
        "  c. Use accuracy as a metric.\n",
        "5. Model Training (0.5 + 0.25 = 0.75 mark)\n",
        "\n",
        "  a. Train the model for an appropriate number of epochs. Print the train and validation accuracy and loss for each epoch. Use the appropriate batch size.\n",
        "\n",
        "  b. Plot the loss and accuracy history graphs for both train and validation set. Print the total time taken for training.\n",
        "6. Model Evaluation (0.5 + 0.5 = 1 mark)\n",
        "\n",
        "  a. Print the final train and validation loss and accuracy. Print confusion matrix and classification report for the validation dataset. Analyse and report the best and worst performing class.\n",
        "  \n",
        "  b. Print the two most incorrectly classified images for each class in the test dataset.\n",
        "\n",
        "  Hyperparameter Tuning- Build two more additional models by changing the following hyperparameters ONE at a time. Write the code for Model Building, Model Compilation, Model Training and Model Evaluation as given in the instructions above for each additional model. (1 + 1 = 2 marks)\n",
        "\n",
        "1. Batch Size: Change the value of batch size in model training\n",
        "\n",
        "2. Optimiser: Use a different optimizer with the appropriate LR value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7AoBVv-xrnM"
      },
      "source": [
        "##Task-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMm401h1xyg_"
      },
      "source": [
        "##Task-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4xaBki3x4C5"
      },
      "source": [
        "##Task-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrFBfZzTx4WX"
      },
      "source": [
        "##Task-4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2xVgEsfx4cX"
      },
      "source": [
        "##Task-5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GleJO4iVx4gj"
      },
      "source": [
        "##Task-6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlaOdZOmx4kX"
      },
      "source": [
        "##Task-7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAUiq0aMx__s"
      },
      "source": [
        "##Task-8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATaT2m_7zhJC"
      },
      "source": [
        "# Question No.2. \n",
        "\n",
        "*NLP Dataset:* IMDB-50K Movie Review dataset comprising 50K movie reviews. Please find your dataset from the link - https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/notebooks (6 marks)\n",
        "\n",
        "##Tasks:\n",
        "\n",
        "1. Import Libraries/Dataset (0 mark)\n",
        "\n",
        "  a. Import the required libraries and the dataset (use Google Drive if required).\n",
        "\n",
        "  b. Check the GPU available (recommended- use free GPU provided by Google Colab).\n",
        "2. Data Visualization (0.75 mark) \n",
        "  \n",
        "  a. Print at least two movie reviews from each class of the dataset, for a sanity check that labels match the text. \n",
        "  \n",
        "  b. Plot a bar graph of class distribution in a dataset. Each bar depicts the number of tweets belonging to a particular sentiment. (recommended - matplotlib/seaborn libraries) \n",
        "  \n",
        "  c. Any other visualizations that seem appropriate for this problem are encouraged but not necessary, for the points.\n",
        "  \n",
        "  d. Print the shapes of train and test data.\n",
        "3. Data Pre-processing (0.25 mark) \n",
        "  \n",
        "  a. Need for this Step - Since the models we use cannot accept string inputs or cannot be of the string format. We have to come up with a way of handling this step. The discussion of different ways of handling this step is out of the scope of this assignment.\n",
        "\n",
        "  b. Please use this pre-trained embedding layer from TensorFlow hub for this assignment. This link also has a code snippet on how to convert a sentence to a vector. Refer to that for further clarity on this subject.\n",
        "\n",
        "  c. Bring the train and test data in the required format.\n",
        "4. Model Building (0.2*5 = 1 mark)\n",
        "\n",
        "  a. Sequential Model layers- Use AT LEAST 3 hidden layers with appropriate input for each. Choose the best number for hidden units and give reasons.\n",
        "\n",
        "  b. Add L2 regularization to all the layers.\n",
        "\n",
        "  c. Add one layer of dropout at the appropriate position and give reasons.\n",
        "\n",
        "  d. Choose the appropriate activation function for all the layers.\n",
        "\n",
        "  e. Print the model summary.\n",
        "5. Model Compilation (0.25 mark)\n",
        "\n",
        "  a. Compile the model with the appropriate loss function.\n",
        "\n",
        "  b. Use an appropriate optimizer. Give reasons for the choice of learning rate and its value.\n",
        "\n",
        "  c. Use accuracy as a metric.\n",
        "6. Model Training (0.5 + 0.25 = 0.75 mark)\n",
        "\n",
        "  a. Train the model for an appropriate number of epochs. Print the train and validation accuracy and loss for each epoch. Use the appropriate batch size.\n",
        "\n",
        "  b. Plot the loss and accuracy history graphs for both train and validation set. Print the total time taken for training.\n",
        "7. Model Evaluation (0.5 + 0.5 = 1 mark)\n",
        "  \n",
        "  a. Print the final train and validation loss and accuracy. Print confusion matrix and classification report for the validation dataset. Analyse and report the best and worst performing class.\n",
        "\n",
        "  b. Print the two most incorrectly classified texts for each class in the test dataset.\n",
        "\n",
        "  Hyperparameter Tuning- Build two more models by changing the following hyperparameters one at a time. Write the code for Model Building, Model Compilation, Model Training and Model Evaluation as given in the instructions above for each additional model. (1 + 1 = 2 marks)\n",
        "\n",
        "8. Network Depth: Change the number of hidden layers and hidden units for each layer\n",
        "9. Regularization: Train a model without regularization\n",
        "Write a comparison between each model and give reasons for the difference in results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFZFM200JpP"
      },
      "source": [
        "##Task-1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZSD4GtB0fAh"
      },
      "source": [
        "##Task-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA-9Qvtx0gZD"
      },
      "source": [
        "##Task-3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXWPEZ9W0ghK"
      },
      "source": [
        "##Task-4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ8FLimV0glg"
      },
      "source": [
        "##Task-5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtFVcghe0go0"
      },
      "source": [
        "##Task-6\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn5HyAcT0gsV"
      },
      "source": [
        "##Task-7\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jz4Z48y0gwQ"
      },
      "source": [
        "##Task-8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSlRkF1N0n2-"
      },
      "source": [
        "##Task-9\n",
        "\n"
      ]
    }
  ]
}